noel@Azure:~/kafka/charts$ helm upgrade --install zookeeper bitnami/zookeeper -n hcm-datatransfer -f values-zookeeper.yaml --debug
history.go:56: [debug] getting history for release zookeeper
Release "zookeeper" does not exist. Installing it now.
install.go:178: [debug] Original chart version: ""
install.go:195: [debug] CHART PATH: /home/noel/.cache/helm/repository/zookeeper-9.1.3.tgz

client.go:128: [debug] creating 4 resource(s)
NAME: zookeeper
LAST DEPLOYED: Tue Sep 20 16:00:51 2022
NAMESPACE: hcm-datatransfer
STATUS: deployed
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
affinity: {}
args: []
auth:
  clientPassword: ""
  clientUser: ""
  enabled: false
  existingSecret: ""
  serverPasswords: ""
  serverUsers: ""
autopurge:
  purgeInterval: 0
  snapRetainCount: 3
clusterDomain: cluster.local
command:
- /scripts/setup.sh
commonAnnotations: {}
commonLabels: {}
configuration: ""
containerPorts:
  client: 2181
  election: 3888
  follower: 2888
  tls: 3181
containerSecurityContext:
  enabled: true
  runAsNonRoot: true
  runAsUser: 1001
customLivenessProbe: {}
customReadinessProbe: {}
customStartupProbe: {}
dataLogDir: ""
diagnosticMode:
  args:
  - infinity
  command:
  - sleep
  enabled: false
existingConfigmap: ""
extraDeploy: []
extraEnvVars: []
extraEnvVarsCM: ""
extraEnvVarsSecret: ""
extraVolumeMounts: []
extraVolumes: []
fourlwCommandsWhitelist: srvr, mntr, ruok
fullnameOverride: ""
global:
  imagePullSecrets: []
  imageRegistry: ""
  storageClass: ""
heapSize: 1024
hostAliases: []
image:
  debug: false
  pullPolicy: IfNotPresent
  pullSecrets: []
  registry: docker.io
  repository: bitnami/zookeeper
  tag: 3.8.0-debian-10-r61
initContainers: []
initLimit: 10
jvmFlags: ""
kubeVersion: ""
lifecycleHooks: {}
listenOnAllIPs: false
livenessProbe:
  enabled: true
  failureThreshold: 6
  initialDelaySeconds: 30
  periodSeconds: 10
  probeCommandTimeout: 2
  successThreshold: 1
  timeoutSeconds: 5
logLevel: ERROR
maxClientCnxns: 60
maxSessionTimeout: 40000
metrics:
  containerPort: 9141
  enabled: false
  prometheusRule:
    additionalLabels: {}
    enabled: false
    namespace: ""
    rules: []
  service:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: '{{ .Values.metrics.service.port }}'
      prometheus.io/scrape: "true"
    port: 9141
    type: ClusterIP
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    honorLabels: false
    interval: ""
    jobLabel: ""
    metricRelabelings: []
    namespace: ""
    relabelings: []
    scrapeTimeout: ""
    selector: {}
minServerId: 1
nameOverride: ""
namespaceOverride: ""
networkPolicy:
  allowExternal: true
  enabled: false
nodeAffinityPreset:
  key: ""
  type: ""
  values: []
nodeSelector: {}
pdb:
  create: false
  maxUnavailable: 1
  minAvailable: ""
persistence:
  accessModes:
  - ReadWriteOnce
  annotations: {}
  dataLogDir:
    existingClaim: ""
    selector: {}
    size: 8Gi
  enabled: true
  existingClaim: ""
  selector: {}
  size: 8Gi
  storageClass: ""
podAffinityPreset: ""
podAnnotations: {}
podAntiAffinityPreset: soft
podLabels: {}
podManagementPolicy: Parallel
podSecurityContext:
  enabled: true
  fsGroup: 1001
preAllocSize: 65536
priorityClassName: ""
readinessProbe:
  enabled: true
  failureThreshold: 6
  initialDelaySeconds: 5
  periodSeconds: 10
  probeCommandTimeout: 2
  successThreshold: 1
  timeoutSeconds: 5
replicaCount: 3
resources:
  limits: {}
  requests:
    cpu: 250m
    memory: 256Mi
schedulerName: ""
service:
  annotations: {}
  clusterIP: ""
  disableBaseClientPort: false
  externalTrafficPolicy: Cluster
  extraPorts: []
  headless:
    annotations: {}
    publishNotReadyAddresses: true
  loadBalancerIP: ""
  loadBalancerSourceRanges: []
  nodePorts:
    client: ""
    tls: ""
  ports:
    client: 2181
    election: 3888
    follower: 2888
    tls: 3181
  sessionAffinity: None
  type: ClusterIP
serviceAccount:
  annotations: {}
  automountServiceAccountToken: true
  create: false
  name: ""
sidecars: []
snapCount: 100000
startupProbe:
  enabled: false
  failureThreshold: 15
  initialDelaySeconds: 30
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1
syncLimit: 5
tickTime: 2000
tls:
  client:
    auth: none
    autoGenerated: false
    enabled: false
    existingSecret: ""
    existingSecretKeystoreKey: ""
    existingSecretTruststoreKey: ""
    keystorePassword: ""
    keystorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.keystore.jks
    passwordsSecretKeystoreKey: ""
    passwordsSecretName: ""
    passwordsSecretTruststoreKey: ""
    truststorePassword: ""
    truststorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.truststore.jks
  quorum:
    auth: none
    autoGenerated: false
    enabled: false
    existingSecret: ""
    existingSecretKeystoreKey: ""
    existingSecretTruststoreKey: ""
    keystorePassword: ""
    keystorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.keystore.jks
    passwordsSecretKeystoreKey: ""
    passwordsSecretName: ""
    passwordsSecretTruststoreKey: ""
    truststorePassword: ""
    truststorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.truststore.jks
  resources:
    limits: {}
    requests: {}
tolerations: []
topologySpreadConstraints: {}
updateStrategy:
  rollingUpdate: {}
  type: RollingUpdate
volumePermissions:
  containerSecurityContext:
    runAsUser: 0
  enabled: false
  image:
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: bitnami/bitnami-shell
    tag: 10-debian-10-r427
  resources:
    limits: {}
    requests: {}

COMPUTED VALUES:
affinity: {}
args: []
auth:
  clientPassword: ""
  clientUser: ""
  enabled: false
  existingSecret: ""
  serverPasswords: ""
  serverUsers: ""
autopurge:
  purgeInterval: 0
  snapRetainCount: 3
clusterDomain: cluster.local
command:
- /scripts/setup.sh
common:
  exampleValue: common-chart
  global:
    imagePullSecrets: []
    imageRegistry: ""
    storageClass: ""
commonAnnotations: {}
commonLabels: {}
configuration: ""
containerPorts:
  client: 2181
  election: 3888
  follower: 2888
  tls: 3181
containerSecurityContext:
  enabled: true
  runAsNonRoot: true
  runAsUser: 1001
customLivenessProbe: {}
customReadinessProbe: {}
customStartupProbe: {}
dataLogDir: ""
diagnosticMode:
  args:
  - infinity
  command:
  - sleep
  enabled: false
existingConfigmap: ""
extraDeploy: []
extraEnvVars: []
extraEnvVarsCM: ""
extraEnvVarsSecret: ""
extraVolumeMounts: []
extraVolumes: []
fourlwCommandsWhitelist: srvr, mntr, ruok
fullnameOverride: ""
global:
  imagePullSecrets: []
  imageRegistry: ""
  storageClass: ""
heapSize: 1024
hostAliases: []
image:
  debug: false
  pullPolicy: IfNotPresent
  pullSecrets: []
  registry: docker.io
  repository: bitnami/zookeeper
  tag: 3.8.0-debian-10-r61
initContainers: []
initLimit: 10
jvmFlags: ""
kubeVersion: ""
lifecycleHooks: {}
listenOnAllIPs: false
livenessProbe:
  enabled: true
  failureThreshold: 6
  initialDelaySeconds: 30
  periodSeconds: 10
  probeCommandTimeout: 2
  successThreshold: 1
  timeoutSeconds: 5
logLevel: ERROR
maxClientCnxns: 60
maxSessionTimeout: 40000
metrics:
  containerPort: 9141
  enabled: false
  prometheusRule:
    additionalLabels: {}
    enabled: false
    namespace: ""
    rules: []
  service:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: '{{ .Values.metrics.service.port }}'
      prometheus.io/scrape: "true"
    port: 9141
    type: ClusterIP
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    honorLabels: false
    interval: ""
    jobLabel: ""
    metricRelabelings: []
    namespace: ""
    relabelings: []
    scrapeTimeout: ""
    selector: {}
minServerId: 1
nameOverride: ""
namespaceOverride: ""
networkPolicy:
  allowExternal: true
  enabled: false
nodeAffinityPreset:
  key: ""
  type: ""
  values: []
nodeSelector: {}
pdb:
  create: false
  maxUnavailable: 1
  minAvailable: ""
persistence:
  accessModes:
  - ReadWriteOnce
  annotations: {}
  dataLogDir:
    existingClaim: ""
    selector: {}
    size: 8Gi
  enabled: true
  existingClaim: ""
  selector: {}
  size: 8Gi
  storageClass: ""
podAffinityPreset: ""
podAnnotations: {}
podAntiAffinityPreset: soft
podLabels: {}
podManagementPolicy: Parallel
podSecurityContext:
  enabled: true
  fsGroup: 1001
preAllocSize: 65536
priorityClassName: ""
readinessProbe:
  enabled: true
  failureThreshold: 6
  initialDelaySeconds: 5
  periodSeconds: 10
  probeCommandTimeout: 2
  successThreshold: 1
  timeoutSeconds: 5
replicaCount: 3
resources:
  limits: {}
  requests:
    cpu: 250m
    memory: 256Mi
schedulerName: ""
service:
  annotations: {}
  clusterIP: ""
  disableBaseClientPort: false
  externalTrafficPolicy: Cluster
  extraPorts: []
  headless:
    annotations: {}
    publishNotReadyAddresses: true
  loadBalancerIP: ""
  loadBalancerSourceRanges: []
  nodePorts:
    client: ""
    tls: ""
  ports:
    client: 2181
    election: 3888
    follower: 2888
    tls: 3181
  sessionAffinity: None
  type: ClusterIP
serviceAccount:
  annotations: {}
  automountServiceAccountToken: true
  create: false
  name: ""
sidecars: []
snapCount: 100000
startupProbe:
  enabled: false
  failureThreshold: 15
  initialDelaySeconds: 30
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 1
syncLimit: 5
tickTime: 2000
tls:
  client:
    auth: none
    autoGenerated: false
    enabled: false
    existingSecret: ""
    existingSecretKeystoreKey: ""
    existingSecretTruststoreKey: ""
    keystorePassword: ""
    keystorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.keystore.jks
    passwordsSecretKeystoreKey: ""
    passwordsSecretName: ""
    passwordsSecretTruststoreKey: ""
    truststorePassword: ""
    truststorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.truststore.jks
  quorum:
    auth: none
    autoGenerated: false
    enabled: false
    existingSecret: ""
    existingSecretKeystoreKey: ""
    existingSecretTruststoreKey: ""
    keystorePassword: ""
    keystorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.keystore.jks
    passwordsSecretKeystoreKey: ""
    passwordsSecretName: ""
    passwordsSecretTruststoreKey: ""
    truststorePassword: ""
    truststorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.truststore.jks
  resources:
    limits: {}
    requests: {}
tolerations: []
topologySpreadConstraints: {}
updateStrategy:
  rollingUpdate: {}
  type: RollingUpdate
volumePermissions:
  containerSecurityContext:
    runAsUser: 0
  enabled: false
  image:
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: bitnami/bitnami-shell
    tag: 10-debian-10-r427
  resources:
    limits: {}
    requests: {}

HOOKS:
MANIFEST:
---
# Source: zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: zookeeper-scripts
  namespace: hcm-datatransfer
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.1.3
    app.kubernetes.io/instance: zookeeper
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-headless
  namespace: hcm-datatransfer
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.1.3
    app.kubernetes.io/instance: zookeeper
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: zookeeper
    app.kubernetes.io/component: zookeeper
---
# Source: zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  namespace: hcm-datatransfer
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.1.3
    app.kubernetes.io/instance: zookeeper
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: zookeeper
    app.kubernetes.io/component: zookeeper
---
# Source: zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper
  namespace: hcm-datatransfer
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.1.3
    app.kubernetes.io/instance: zookeeper
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 3
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: zookeeper
      app.kubernetes.io/component: zookeeper
  serviceName: zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-9.1.3
        app.kubernetes.io/instance: zookeeper
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default

      affinity:
        podAffinity:

        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/instance: zookeeper
                    app.kubernetes.io/component: zookeeper
                namespaces:
                  - "hcm-datatransfer"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:

      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.0-debian-10-r61
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: zookeeper-0.zookeeper-headless.hcm-datatransfer.svc.cluster.local:2888:3888::1 zookeeper-1.zookeeper-headless.hcm-datatransfer.svc.cluster.local:2888:3888::2 zookeeper-2.zookeeper-headless.hcm-datatransfer.svc.cluster.local:2888:3888::3
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: zookeeper-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"

NOTES:
CHART NAME: zookeeper
CHART VERSION: 9.1.3
APP VERSION: 3.8.0

** Please be patient while the chart is being deployed **

ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster:

    zookeeper.hcm-datatransfer.svc.cluster.local

To connect to your ZooKeeper server run the following commands:

    export POD_NAME=$(kubectl get pods --namespace hcm-datatransfer -l "app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zookeeper,app.kubernetes.io/component=zookeeper" -o jsonpath="{.items[0].metadata.name}")
    kubectl exec -it $POD_NAME -- zkCli.sh

To connect to your ZooKeeper server from outside the cluster execute the following commands:

    kubectl port-forward --namespace hcm-datatransfer svc/zookeeper 2181: &
    zkCli.sh 127.0.0.1:2181