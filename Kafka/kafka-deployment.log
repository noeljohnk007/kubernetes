											        existingSecret: ""
      serverPasswords: ""
      serverUsers: ""
    clientPassword: ""
    clientUser: ""
    enabled: false
    quorum:
      enabled: false
      existingSecret: ""
      learnerPassword: ""
      learnerUser: ""
      serverPasswords: ""
      serverUsers: ""
    serverPasswords: ""
    serverUsers: ""
  autopurge:
    purgeInterval: 0
    snapRetainCount: 3
  clusterDomain: cluster.local
  command:
  - /scripts/setup.sh
  common:
    exampleValue: common-chart
    global:
      imagePullSecrets: []
      imageRegistry: ""
      storageClass: ""
  commonAnnotations: {}
  commonLabels: {}
  configuration: ""
  containerPorts:
    client: 2181
    election: 3888
    follower: 2888
    tls: 3181
  containerSecurityContext:
    allowPrivilegeEscalation: false
    enabled: true
    runAsNonRoot: true
    runAsUser: 1001
  customLivenessProbe: {}
  customReadinessProbe: {}
  customStartupProbe: {}
  dataLogDir: ""
  diagnosticMode:
    args:
    - infinity
    command:
    - sleep
    enabled: false
  enabled: true
  existingConfigmap: ""
  extraDeploy: []
  extraEnvVars: []
  extraEnvVarsCM: ""
  extraEnvVarsSecret: ""
  extraVolumeMounts: []
  extraVolumes: []
  fourlwCommandsWhitelist: srvr, mntr, ruok
  fullnameOverride: ""
  global:
    imagePullSecrets: []
    imageRegistry: ""
    storageClass: ""
  heapSize: 1024
  hostAliases: []
  image:
    debug: false
    digest: ""
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: bitnami/zookeeper
    tag: 3.8.0-debian-11-r36
  initContainers: []
  initLimit: 10
  jvmFlags: ""
  kubeVersion: ""
  lifecycleHooks: {}
  listenOnAllIPs: false
  livenessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 30
    periodSeconds: 10
    probeCommandTimeout: 2
    successThreshold: 1
    timeoutSeconds: 5
  logLevel: ERROR
  maxClientCnxns: 60
  maxSessionTimeout: 40000
  metrics:
    containerPort: 9141
    enabled: false
    prometheusRule:
      additionalLabels: {}
      enabled: false
      namespace: ""
      rules: []
    service:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '{{ .Values.metrics.service.port }}'
        prometheus.io/scrape: "true"
      port: 9141
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      enabled: false
      honorLabels: false
      interval: ""
      jobLabel: ""
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scrapeTimeout: ""
      selector: {}
  minServerId: 1
  nameOverride: ""
  namespaceOverride: ""
  networkPolicy:
    allowExternal: true
    enabled: false
  nodeAffinityPreset:
    key: ""
    type: ""
    values: []
  nodeSelector: {}
  pdb:
    create: false
    maxUnavailable: 1
    minAvailable: ""
  persistence:
    accessModes:
    - ReadWriteOnce
    annotations: {}
    dataLogDir:
      existingClaim: ""
      selector: {}
      size: 8Gi
    enabled: true
    existingClaim: ""
    selector: {}
    size: 8Gi
    storageClass: ""
  podAffinityPreset: ""
  podAnnotations: {}
  podAntiAffinityPreset: soft
  podLabels: {}
  podManagementPolicy: Parallel
  podSecurityContext:
    enabled: true
    fsGroup: 1001
  preAllocSize: 65536
  priorityClassName: ""
  readinessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 5
    periodSeconds: 10
    probeCommandTimeout: 2
    successThreshold: 1
    timeoutSeconds: 5
  replicaCount: 3
  resources:
    limits: {}
    requests:
      cpu: 250m
      memory: 256Mi
  schedulerName: ""
  service:
    annotations: {}
    clusterIP: ""
    disableBaseClientPort: false
    externalTrafficPolicy: Cluster
    extraPorts: []
    headless:
      annotations: {}
      publishNotReadyAddresses: true
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    nodePorts:
      client: ""
      tls: ""
    ports:
      client: 2181
      election: 3888
      follower: 2888
      tls: 3181
    sessionAffinity: None
    sessionAffinityConfig: {}
    type: ClusterIP
  serviceAccount:
    annotations: {}
    automountServiceAccountToken: true
    create: false
    name: ""
  sidecars: []
  snapCount: 100000
  startupProbe:
    enabled: false
    failureThreshold: 15
    initialDelaySeconds: 30
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1
  syncLimit: 5
  tickTime: 2000
  tls:
    client:
      auth: none
      autoGenerated: false
      enabled: false
      existingSecret: ""
      existingSecretKeystoreKey: ""
      existingSecretTruststoreKey: ""
      keystorePassword: ""
      keystorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.keystore.jks
      passwordsSecretKeystoreKey: ""
      passwordsSecretName: ""
      passwordsSecretTruststoreKey: ""
      truststorePassword: ""
      truststorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.truststore.jks
    quorum:
      auth: none
      autoGenerated: false
      enabled: false
      existingSecret: ""
      existingSecretKeystoreKey: ""
      existingSecretTruststoreKey: ""
      keystorePassword: ""
      keystorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.keystore.jks
      passwordsSecretKeystoreKey: ""
      passwordsSecretName: ""
      passwordsSecretTruststoreKey: ""
      truststorePassword: ""
      truststorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.truststore.jks
    resources:
      limits: {}
      requests: {}
  tolerations: []
  topologySpreadConstraints: []
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  volumePermissions:
    containerSecurityContext:
      runAsUser: 0
    enabled: false
    image:
      digest: ""
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 11-debian-11-r35
    resources:
      limits: {}
      requests: {}
zookeeperChrootPath: ""
zookeeperConnectionTimeoutMs: 6000

HOOKS:
MANIFEST:
---
# Source: kafka/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kafka
  namespace: "hcm-datatransfer"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-19.0.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
  annotations:
automountServiceAccountToken: true
---
# Source: kafka/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-zookeeper-scripts
  namespace: hcm-datatransfer
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.2.2
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-scripts
  namespace: "hcm-datatransfer"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-19.0.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
data:
  auto-discovery.sh: |-
    #!/bin/bash

    SVC_NAME="${MY_POD_NAME}-external"
    # Auxiliary functions
    retry_while() {
        local -r cmd="${1:?cmd is missing}"
        local -r retries="${2:-12}"
        local -r sleep_time="${3:-5}"
        local return_value=1

        read -r -a command <<< "$cmd"
        for ((i = 1 ; i <= retries ; i+=1 )); do
            "${command[@]}" && return_value=0 && break
            sleep "$sleep_time"
        done
        return $return_value
    }
    k8s_svc_lb_ip() {
        local namespace=${1:?namespace is missing}
        local service=${2:?service is missing}
        local service_ip=$(kubectl get svc "$service" -n "$namespace" -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
        local service_hostname=$(kubectl get svc "$service" -n "$namespace" -o jsonpath="{.status.loadBalancer.ingress[0].hostname}")

        if [[ -n ${service_ip} ]]; then
            echo "${service_ip}"
        else
            echo "${service_hostname}"
        fi
    }
    k8s_svc_lb_ip_ready() {
        local namespace=${1:?namespace is missing}
        local service=${2:?service is missing}
        [[ -n "$(k8s_svc_lb_ip "$namespace" "$service")" ]]
    }
    # Wait until LoadBalancer IP is ready
    retry_while "k8s_svc_lb_ip_ready hcm-datatransfer $SVC_NAME" || exit 1
    # Obtain LoadBalancer external IP
    k8s_svc_lb_ip "hcm-datatransfer" "$SVC_NAME" | tee "$SHARED_FILE"
  setup.sh: |-
    #!/bin/bash

    ID="${MY_POD_NAME#"kafka-"}"
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        export KAFKA_CFG_BROKER_ID="$(grep "broker.id" "/bitnami/kafka/data/meta.properties" | awk -F '=' '{print $2}')"
    else
        export KAFKA_CFG_BROKER_ID="$((ID + 0))"
    fi
    # Configure external ip and port
    export EXTERNAL_ACCESS_HOST="$(<${SHARED_FILE})"
    export EXTERNAL_ACCESS_PORT=9094

    # Configure Kafka advertised listeners
    export KAFKA_CFG_ADVERTISED_LISTENERS="INTERNAL://${MY_POD_NAME}.kafka-headless.hcm-datatransfer.svc.cluster.local:9093,CLIENT://${MY_POD_NAME}.kafka-headless.hcm-datatransfer.svc.cluster.local:9092,EXTERNAL://${EXTERNAL_ACCESS_HOST}:${EXTERNAL_ACCESS_PORT}"

    # Configure zookeeper client

    exec /entrypoint.sh /run.sh
---
# Source: kafka/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kafka
  namespace: "hcm-datatransfer"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-19.0.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
rules:
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
---
# Source: kafka/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kafka
  namespace: "hcm-datatransfer"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-19.0.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
roleRef:
  kind: Role
  name: kafka
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: kafka
    namespace: hcm-datatransfer
---
# Source: kafka/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-zookeeper-headless
  namespace: hcm-datatransfer
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.2.2
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/component: zookeeper
---
# Source: kafka/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-zookeeper
  namespace: hcm-datatransfer
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.2.2
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/component: zookeeper
---
# Source: kafka/templates/svc-external-access.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-0-external
  namespace: "hcm-datatransfer"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-19.0.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
    pod: kafka-0
spec:
  type: LoadBalancer
  ports:
    - name: tcp-kafka
      port: 9094
      nodePort: null
      targetPort: kafka-external
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/component: kafka
    statefulset.kubernetes.io/pod-name: kafka-0
---
# Source: kafka/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-headless
  namespace: "hcm-datatransfer"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-19.0.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
    - name: tcp-internal
      port: 9093
      protocol: TCP
      targetPort: kafka-internal
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/component: kafka
---
# Source: kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: "hcm-datatransfer"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-19.0.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
      nodePort: null
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/component: kafka
---
# Source: kafka/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka-zookeeper
  namespace: hcm-datatransfer
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-10.2.2
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 3
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/component: zookeeper
  serviceName: kafka-zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-10.2.2
        app.kubernetes.io/instance: kafka
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/instance: kafka
                    app.kubernetes.io/component: zookeeper
                namespaces:
                  - "hcm-datatransfer"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.0-debian-11-r36
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: kafka-zookeeper-0.kafka-zookeeper-headless.hcm-datatransfer.svc.cluster.local:2888:3888::1 kafka-zookeeper-1.kafka-zookeeper-headless.hcm-datatransfer.svc.cluster.local:2888:3888::2 kafka-zookeeper-2.kafka-zookeeper-headless.hcm-datatransfer.svc.cluster.local:2888:3888::3 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: kafka-zookeeper-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: "hcm-datatransfer"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-19.0.0
    app.kubernetes.io/instance: kafka
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: kafka
      app.kubernetes.io/component: kafka
  serviceName: kafka-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-19.0.0
        app.kubernetes.io/instance: kafka
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka
      annotations:
        checksum/scripts: de08f0bc8d93ca8c4b53ccd578c2e17c77e2d52c327c3ef87978eb6bb3d5ab75
    spec:
      
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/instance: kafka
                    app.kubernetes.io/component: kafka
                namespaces:
                  - "hcm-datatransfer"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: kafka
      initContainers:
        - name: auto-discovery
          image: docker.io/bitnami/kubectl:1.23.6-debian-10-r22
          imagePullPolicy: "IfNotPresent"
          command:
            - /scripts/auto-discovery.sh
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: SHARED_FILE
              value: "/shared/info.txt"
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: shared
              mountPath: /shared
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/auto-discovery.sh
              subPath: auto-discovery.sh
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.1.1-debian-10-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: "kafka-zookeeper"
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: "INTERNAL"
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: "INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT"
            - name: KAFKA_CFG_LISTENERS
              value: "INTERNAL://:9093,CLIENT://:9092,EXTERNAL://:9094"
            - name: SHARED_FILE
              value: "/shared/info.txt"
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_ZOOKEEPER_PROTOCOL
              value: PLAINTEXT
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_LOG_DIR
              value: "/opt/bitnami/kafka/logs"
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "true"
            - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
              value: "true"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVAL_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "168"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: "/bitnami/kafka/data"
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "1"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
            - name: KAFKA_CFG_AUTHORIZER_CLASS_NAME
              value: ""
            - name: KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND
              value: "true"
            - name: KAFKA_CFG_SUPER_USERS
              value: "User:admin"
          ports:
            - name: kafka-client
              containerPort: 9092
            - name: kafka-internal
              containerPort: 9093
            - name: kafka-external
              containerPort: 9094
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: shared
              mountPath: /shared
      volumes:
        - name: scripts
          configMap:
            name: kafka-scripts
            defaultMode: 0755
        - name: shared
          emptyDir: {}
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"

NOTES:
CHART NAME: kafka
CHART VERSION: 19.0.0
APP VERSION: 3.3.1
---------------------------------------------------------------------------------------------
 WARNING

    By specifying "serviceType=LoadBalancer" and not configuring the authentication
    you have most likely exposed the Kafka service externally without any
    authentication mechanism.

    For security reasons, we strongly suggest that you switch to "ClusterIP" or
    "NodePort". As alternative, you can also configure the Kafka authentication.

---------------------------------------------------------------------------------------------

** Please be patient while the chart is being deployed **

Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:

    kafka.hcm-datatransfer.svc.cluster.local

Each Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster:

    kafka-0.kafka-headless.hcm-datatransfer.svc.cluster.local:9092

To create a pod that you can use as a Kafka client run the following commands:

    kubectl run kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.1.1-debian-10-r0 --namespace hcm-datatransfer --command -- sleep infinity
    kubectl exec --tty -i kafka-client --namespace hcm-datatransfer -- bash

    PRODUCER:
        kafka-console-producer.sh \
            
            --broker-list kafka-0.kafka-headless.hcm-datatransfer.svc.cluster.local:9092 \
            --topic test

    CONSUMER:
        kafka-console-consumer.sh \
            
            --bootstrap-server kafka.hcm-datatransfer.svc.cluster.local:9092 \
            --topic test \
            --from-beginning

To connect to your Kafka server from outside the cluster, follow the instructions below:

  NOTE: It may take a few minutes for the LoadBalancer IPs to be available.
        Watch the status with: 'kubectl get svc --namespace hcm-datatransfer -l "app.kubernetes.io/name=kafka,app.kubernetes.io/instance=kafka,app.kubernetes.io/component=kafka,pod" -w'

    Kafka Brokers domain: You will have a different external IP for each Kafka broker. You can get the list of external IPs using the command below:

        echo "$(kubectl get svc --namespace hcm-datatransfer -l "app.kubernetes.io/name=kafka,app.kubernetes.io/instance=kafka,app.kubernetes.io/component=kafka,pod" -o jsonpath='{.items[*].status.loadBalancer.ingress[0].ip}' | tr ' ' '\n')"

    Kafka Brokers port: 9094
noel@Azure:~/kafka/charts$ 